{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center> Features: TF-IDF, N-grams and word2vec </center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "file = \"../../data/AirlineSentiment.csv\"\n",
    "sent_data = pd.read_csv(file)\n",
    "sent_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet text\n",
    "text = sent_data.values[:, 14]\n",
    "# sentiment\n",
    "sent = sent_data.values[:, 5]\n",
    "\n",
    "# remove non-alphanumeric characters\n",
    "text = [re.sub(r'\\W+', ' ', x) for x in text]\n",
    "\n",
    "# tokenize\n",
    "text_tokens_full = [nltk.word_tokenize(x.lower()) for x in text]\n",
    "\n",
    "# stem\n",
    "stemmer = PorterStemmer()\n",
    "text_tokens = [[stemmer.stem(w) for w in x] for x in text_tokens_full]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, see how features are represented with binary bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of TfidfVectorizer; get it to remove stopwords\n",
    "tf = TfidfVectorizer(stop_words = 'english', use_idf = False, norm = None, binary = True, lowercase = True)\n",
    "\n",
    "# convert text to features\n",
    "text_tokens_feat = tf.fit_transform([' '.join(x) for x in text_tokens])\n",
    "\n",
    "# save feature names\n",
    "feature_names = tf.get_feature_names()\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "feature_matrix = pd.DataFrame(text_tokens_feat.toarray()[:10], columns = feature_names)\n",
    "\n",
    "# print the first several examples and the features\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise. Represent features with tf-idf\n",
    "\n",
    "Use `TfidfVectorizer` to create features represented with tf-idf weights. To do so, initialize this class with the following parameters:\n",
    "* `use_idf: True`\n",
    "* `norm: 'l2'`\n",
    "* `binary: False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of TfidfVectorizer with tf-idf; also get it to remove stopwords, convert to lowercase\n",
    "tf = TfidfVectorizer(stop_words = 'english', use_idf = True, norm = 'l2', binary = False, lowercase = True)\n",
    "\n",
    "# convert text to features\n",
    "text_tokens_tfidf = tf.fit_transform([' '.join(x) for x in text_tokens])\n",
    "\n",
    "# get feature names\n",
    "feature_names = tf.get_feature_names()\n",
    "\n",
    "# print the first several examples and the features\n",
    "feature_matrix = pd.DataFrame(text_tokens_tfidf.toarray()[:10], columns = feature_names)\n",
    "\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise. Convert labels to numbers\n",
    "\n",
    "Sentiment in the `sent` variable is now represented with words. Use `LabelEncoder` to convert it into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "le = LabelEncoder()\n",
    "sent = le.fit_transform(sent)\n",
    "class_labels = le.classes_\n",
    "[print(class_labels[x], x) for x in sent[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise. Train the model\n",
    "\n",
    "Perform a train / test split and train a `LinearSVC` model with features represented as tf-idf. Then, calculate the `f1` and `accuracy` scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(text_tokens_tfidf, sent, random_state = 0)\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_tfidf = clf.predict(X_test_tfidf)\n",
    "f1_tfidf = f1_score(y_test_tfidf, y_pred_tfidf, average = \"macro\")\n",
    "accuracy_tfidf = accuracy_score(y_test_tfidf, y_pred_tfidf)\n",
    "print(\"SVM f1 score:\", f1_tfidf, \"accuracy:\", accuracy_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. N-grams\n",
    "\n",
    "### 2.1. Bigrams\n",
    "\n",
    "#### Exercise. Add bigram features\n",
    "\n",
    "Bigrams are two-token features. For example, in a sentence `\"Today is sunny\"`, `Today is` and `is sunny` are bigrams.\n",
    "\n",
    "Use `TfidfVectorizer` to add bigram features. The parameter that controls this is `ngram_range`; use both 1-grams and bigrams. Have the vectorizer output tf-idf weights. You may also want to limit the number of features produced. This is controlled by `max_features` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of TfidfVectorizer with 2-grams; also get it to remove stopwords, convert to lowercase\n",
    "tf = TfidfVectorizer(stop_words = 'english', ngram_range = (1, 2), use_idf = True, norm = 'l2', \n",
    "                     binary = False, lowercase = True, max_features = 30000)\n",
    "\n",
    "# convert text to features\n",
    "text_tokens_bigram = tf.fit_transform([' '.join(x) for x in text_tokens])\n",
    "\n",
    "# get feature names\n",
    "feature_names = tf.get_feature_names()\n",
    "\n",
    "# print dimensions of the feature matrix\n",
    "print(\"Examples, features:\", text_tokens_bigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first few feature names\n",
    "feature_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise. Write a function that trains a `LinearSVC` model and prints out accuracy scores\n",
    "\n",
    "Write a function that:\n",
    "* Takes features and labels as input\n",
    "* Performs train/test split\n",
    "* Trains a `LinearSVC` model\n",
    "* Calculates `f1` and `accuracy` scores and prints them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svc_model(text_tokens, sent):\n",
    "    \"\"\"\n",
    "    Train a LinearSVC model and print out f1 and accuracy scores\n",
    "    \n",
    "    Inputs:\n",
    "    text_tokens - features\n",
    "    sent - labels\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(text_tokens, sent, random_state = 0)\n",
    "    clf = LinearSVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average = \"macro\")\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"SVM f1 score:\", f1, \"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your `train_svc_model()` function to train a model with bigram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bigrams\")\n",
    "train_svc_model(text_tokens_bigram, sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.2. Trigrams\n",
    "\n",
    "#### Exercise. Add 3-gram features\n",
    "\n",
    "As before, use `TfidfVectorizer` and its `ngram_range` parameter to create 1-, 2- and 3-grams with tf-idf weights. Again, you may want to limit the number of features created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of TfidfVectorizer with 3-grams; also get it to remove stopwords, convert to lowercase\n",
    "tf = TfidfVectorizer(stop_words = 'english', ngram_range = (1, 3), use_idf = True, norm = 'l2', binary = False, \n",
    "                     lowercase = True, max_features = 80000)\n",
    "\n",
    "# convert text to features\n",
    "text_tokens_trigram = tf.fit_transform([' '.join(x) for x in text_tokens])\n",
    "\n",
    "# get feature names\n",
    "feature_names = tf.get_feature_names()\n",
    "\n",
    "# dimensions of the feature matrix\n",
    "print(\"Examples, features:\", text_tokens_trigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first few feature names\n",
    "feature_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `train_svc_model()` to train a model with the new features. How is the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trigrams\")\n",
    "train_svc_model(text_tokens_trigram, sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# load pretrained vectors from file\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(\"../../data/word2vec50tokens.bin\", binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.word_vec(\"good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise. Find words most similar to `great`\n",
    "\n",
    "Use `similar_by_word()` function of the `w2v` object to find words that are most similar to `great`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "w2v.similar_by_word(\"great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
