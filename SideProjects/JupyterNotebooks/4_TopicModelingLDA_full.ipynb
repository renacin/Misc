{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center> Topic Modeling with LDA and NMF </center>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using a dataset of articles from a newspaper in Kenya, \"The Daily Nation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fileName = \"../../data/NationMediaArticles.csv\"\n",
    "news = pd.read_csv(fileName)\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the data\n",
    "\n",
    "Stem, lemmatize and remove punctuation and other non-alphanumeric characters.\n",
    "\n",
    "First we'll declare some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    \"\"\"\n",
    "    Lemmatizes a word\n",
    "    \"\"\"\n",
    "    # get part of speech (needed for the lemmatizer)\n",
    "    pos = nltk.pos_tag([word])[0][1]\n",
    "    \n",
    "    # convert into wordnet POS\n",
    "    if pos.startswith(\"V\"):\n",
    "        pos_wn = wordnet.VERB\n",
    "    elif pos.startswith(\"R\"):\n",
    "        pos_wn = wordnet.ADV\n",
    "    elif pos.startswith(\"J\"):\n",
    "        pos_wn = wordnet.ADJ\n",
    "    else:\n",
    "        pos_wn = wordnet.NOUN\n",
    "    \n",
    "    # lemmatize\n",
    "    return lm.lemmatize(word, pos = pos_wn)\n",
    "\n",
    "\n",
    "def clean_string(string, lemmas = True):\n",
    "    \"\"\"\n",
    "    Converts the string to lowercase, lemmatizes and removes non-alphanumerics\n",
    "    \"\"\"\n",
    "    if pd.isnull(string): return \"\"\n",
    "    # remove non-alphanumeric characters\n",
    "    string = re.sub(r'[^A-Za-z]+', ' ', string)\n",
    "    # to lowercase and stem / lemmatize\n",
    "    if lemmas:\n",
    "        string = [lemmatize(x) for x in string.lower().split()]\n",
    "    else:\n",
    "        string = [ps.stem(x) for x in string.lower().split()]\n",
    "    return \" \".join(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, clean the data.\n",
    "\n",
    "`lemmas = False` will produce stems. You can pass `lemmas = True` to have `clean_string()` produce lemmas instead of stems, but this is a slower process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean\n",
    "title_cleaned = [clean_string(x, lemmas = False) for x in news.headline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the cleaned data\n",
    "title_cleaned[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create term-document matrices\n",
    "\n",
    "Create term-document matrices from the documents. We'll use two types of weights: tf-idf and binary (bag-of-words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tf-idf and bag-of-words representations\n",
    "\n",
    "max_features = 10000\n",
    "\n",
    "# tf-idf\n",
    "tf = TfidfVectorizer(max_df = 0.95, min_df = 2, max_features = max_features, stop_words = 'english')\n",
    "features_tfidf = tf.fit_transform(title_cleaned)\n",
    "feature_names_tfidf = tf.get_feature_names()\n",
    "\n",
    "# bag-of-words\n",
    "bow = CountVectorizer(max_df = 0.95, min_df = 2, max_features = max_features, stop_words = 'english')\n",
    "features_bow = bow.fit_transform(title_cleaned)\n",
    "feature_names_bow = bow.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the models\n",
    "\n",
    "We'll train two types of topic models: Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF). They both achieve the same result (discover topics in the documents), but while LDA uses a probabilistic approach, NMF uses linear algebra. NMF also may produce more meaningful topics on smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the number of topics in the documents\n",
    "topics_count = 5\n",
    "\n",
    "# train an NMF model\n",
    "nmf = NMF(n_components = topics_count, random_state = 0, alpha = 0.1, l1_ratio = 0.5, init = \"nndsvd\")\n",
    "nmf.fit(features_bow)\n",
    "\n",
    "# train an LDA model\n",
    "lda = LatentDirichletAllocation(n_components = topics_count, max_iter = 5, random_state = 0, learning_method = \"online\")\n",
    "lda.fit(features_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Display topics\n",
    "\n",
    "We'll use a helper function that displays the top terms associated with each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display topics\n",
    "\n",
    "def show_topics(model, feature_names, top_words = 10):\n",
    "    \"\"\"\n",
    "    Displays the top words from a model\n",
    "    \"\"\"\n",
    "    print(\"Model: %s\" % model.__class__.__name__)\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print(\"Topic %i\\n%r\\n\" % (i, \", \".join([feature_names[x] for x in topic.argsort()[:-top_words - 1: -1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the topics obtained from the NMF model\n",
    "show_topics(nmf, feature_names_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print LDA topics\n",
    "show_topics(lda, feature_names_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercise. Train a topic model on the BBC news dataset\n",
    "\n",
    "Use a dataset of news from the BBC to train your own topic model: LDA or NMF, or both. The news items in the BBC dataset are grouped into five categories:\n",
    "* business\n",
    "* tech\n",
    "* entertainment\n",
    "* sport\n",
    "* politics\n",
    "\n",
    "The dataset is in `'../../data/bbc.csv'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "fileName = \"../../data/bbc.csv\"\n",
    "bbc = pd.read_csv(fileName)\n",
    "bbc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "content_cleaned = [clean_string(x, lemmas = False) for x in bbc.content]\n",
    "content_cleaned[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create term-document matrices\n",
    "\n",
    "max_features = 10000\n",
    "\n",
    "# tf-idf\n",
    "tf = TfidfVectorizer(max_df = 0.95, min_df = 2, max_features = max_features, stop_words = 'english')\n",
    "features_tfidf = tf.fit_transform(content_cleaned)\n",
    "feature_names_tfidf = tf.get_feature_names()\n",
    "\n",
    "# bag-of-words\n",
    "bow = CountVectorizer(max_df = 0.95, min_df = 2, max_features = max_features, stop_words = 'english')\n",
    "features_bow = bow.fit_transform(content_cleaned)\n",
    "feature_names_bow = bow.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the models\n",
    "# specify the number of topics in the documents\n",
    "topics_count = 5\n",
    "\n",
    "# train an NMF model\n",
    "nmf = NMF(n_components = topics_count, random_state = 0, alpha = 0.1, l1_ratio = 0.5, init = \"nndsvd\")\n",
    "nmf.fit(features_bow)\n",
    "\n",
    "# train an LDA model\n",
    "lda = LatentDirichletAllocation(n_components = topics_count, max_iter = 5, random_state = 0, learning_method = \"online\")\n",
    "lda.fit(features_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the topics: NMF model\n",
    "show_topics(nmf, feature_names_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the topics: LDA model\n",
    "show_topics(lda, feature_names_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
